{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxgyXj3Ocb-1"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os, re\n",
        "\n",
        "def save_df(df: pd.DataFrame, name: str, directory: str = \".\") -> None:\n",
        "    # ensure directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # create full path\n",
        "    filename = os.path.join(directory, f\"{name}.csv\")\n",
        "\n",
        "    try:\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"✅ Saved DataFrame to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to save DataFrame: {e}\")\n",
        "\n",
        "\n",
        "def load_csv_to_df(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load a CSV file into a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_path}' not found.\")\n",
        "        sys.exit(1)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: File '{csv_path}' is empty.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def reshape_rubric(df):\n",
        "    # rename rubric column\n",
        "    df = df.rename(columns={df.columns[0]: \"rubric\"})\n",
        "\n",
        "    # known model order cycling through triplets\n",
        "    models = [\"gemini\", \"grok\", \"chatgpt\"]\n",
        "\n",
        "    # extract graph types from non-\"Unnamed\" columns\n",
        "    graph_cols = [c for c in df.columns if not re.match(r\"Unnamed\", c) and c != \"rubric\"]\n",
        "\n",
        "    long_rows = []\n",
        "\n",
        "    for graph in graph_cols:\n",
        "        # find this graph's 3 column window (graph + next 2 cols)\n",
        "        idx = df.columns.get_loc(graph)\n",
        "        triplet = df.columns[idx:idx+3]  # assumes consistent structure\n",
        "\n",
        "        for model, col in zip(models, triplet):\n",
        "            temp = df[[\"rubric\", col]].copy()\n",
        "            temp[\"graph_type\"] = graph\n",
        "            temp[\"model\"] = model\n",
        "            temp = temp.rename(columns={col: \"value\"})\n",
        "            long_rows.append(temp)\n",
        "\n",
        "    tidy_df = pd.concat(long_rows, ignore_index=True)\n",
        "    return tidy_df\n",
        "\n",
        "def explode_prompt_types(df):\n",
        "    prompt_types = [\"baseline\", \"selfcheck\", \"standards\"]\n",
        "\n",
        "    verdict_map = {\n",
        "        \"y\": \"YES\",\n",
        "        \"n\": \"NO\",\n",
        "        \"-\": \"N/A\"\n",
        "    }\n",
        "\n",
        "    long_rows = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        val = str(row[\"value\"])  # original value column\n",
        "\n",
        "        if len(val) != 3:\n",
        "            raise ValueError(f\"Value '{val}' does not have 3 characters.\")\n",
        "\n",
        "        for char, pt in zip(val.lower(), prompt_types):  # lower-case normalize\n",
        "            if char not in verdict_map:\n",
        "                raise ValueError(f\"Unexpected verdict character '{char}' in '{val}'\")\n",
        "\n",
        "            new_row = row.copy()\n",
        "            new_row[\"verdict\"] = verdict_map[char]\n",
        "            new_row[\"prompt_type\"] = pt\n",
        "            new_row = new_row.drop(labels=[\"value\"])  # remove old column\n",
        "            long_rows.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(long_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2qNJ36zc5nX",
        "outputId": "c89605bc-d3d8-4b6a-dbf2-3bb8d634eece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               rubric    graph_type    model prompt_type  \\\n",
            "0     Code validity & reproducibility          Bars   gemini    baseline   \n",
            "1     Code validity & reproducibility          Bars   gemini   selfcheck   \n",
            "2     Code validity & reproducibility          Bars   gemini   standards   \n",
            "3        Encoding choice matches task          Bars   gemini    baseline   \n",
            "4        Encoding choice matches task          Bars   gemini   selfcheck   \n",
            "...                               ...           ...      ...         ...   \n",
            "1075            Task intent adherence  stacked bars  chatgpt   selfcheck   \n",
            "1076            Task intent adherence  stacked bars  chatgpt   standards   \n",
            "1077             Holistic readability  stacked bars  chatgpt    baseline   \n",
            "1078             Holistic readability  stacked bars  chatgpt   selfcheck   \n",
            "1079             Holistic readability  stacked bars  chatgpt   standards   \n",
            "\n",
            "     verdict  \n",
            "0        YES  \n",
            "1        YES  \n",
            "2        YES  \n",
            "3        YES  \n",
            "4        YES  \n",
            "...      ...  \n",
            "1075     YES  \n",
            "1076     YES  \n",
            "1077     YES  \n",
            "1078     YES  \n",
            "1079     YES  \n",
            "\n",
            "[1080 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "df = load_csv_to_df(\"LLM_Plotting_Rubric.csv\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl6dz5JGdDbZ",
        "outputId": "39c3c703-28b9-43f5-f28e-1bd0032539b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved DataFrame to ./LLM_Plotting_Rubric.csv\n"
          ]
        }
      ],
      "source": [
        "save_df(df, \"LLM_Plotting_Rubric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSW6ZgS5heTv",
        "outputId": "b9a35e7f-c89a-4ddf-d12a-2128bf43a42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "OVERALL SUCCESS RATE\n",
            "================================================================================\n",
            "Overall success rate: 0.684 (68.4%)\n",
            "\n",
            "================================================================================\n",
            "HARDEST / EASIEST RUBRICS\n",
            "================================================================================\n",
            "Bottom 5 (hardest):\n",
            "rubric\n",
            "Uncertainty depiction (when applicable)    0.000000\n",
            "Axis integrity                             0.361111\n",
            "Legend–encoding alignment                  0.402778\n",
            "Redundancy                                 0.500000\n",
            "Appropriate Data‑ink ratio                 0.652778\n",
            "\n",
            "Top 5 (easiest):\n",
            "rubric\n",
            "Gridlines/ticks & sizing           0.805556\n",
            "Data faithfulness                  0.847222\n",
            "Encoding choice matches task       0.972222\n",
            "Code validity & reproducibility    0.972222\n",
            "Holistic readability               0.972222\n",
            "\n",
            "================================================================================\n",
            "HARDEST / EASIEST GRAPH TYPES\n",
            "================================================================================\n",
            "Hardest graph types:\n",
            "graph_type\n",
            "dual axis       0.503704\n",
            "small multi     0.577778\n",
            "heatmap         0.629630\n",
            "line gaps       0.696296\n",
            "stacked bars    0.725926\n",
            "\n",
            "Easiest graph types:\n",
            "graph_type\n",
            "line gaps        0.696296\n",
            "stacked bars     0.725926\n",
            "scatter group    0.762963\n",
            "Bars             0.777778\n",
            "histogram        0.800000\n",
            "\n",
            "================================================================================\n",
            "BEST PERFORMING MODELS\n",
            "================================================================================\n",
            "model\n",
            "chatgpt    0.672222\n",
            "grok       0.672222\n",
            "gemini     0.708333\n",
            "\n",
            "================================================================================\n",
            "PROMPT TYPE EFFECTS\n",
            "================================================================================\n",
            "prompt_type\n",
            "baseline     0.658333\n",
            "selfcheck    0.688889\n",
            "standards    0.705556\n",
            "\n",
            "================================================================================\n",
            "STATISTICAL SIGNIFICANCE: PROMPT TYPE EFFECT\n",
            "================================================================================\n",
            "Chi-square test on prompt_type vs success:\n",
            "Chi² = 1.911, df = 2, p = 0.38454\n",
            "❌ No statistically significant evidence that prompt type affects success.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Convert verdict to binary success for analysis (ignore N/A)\n",
        "df_eval = df[df[\"verdict\"] != \"N/A\"].copy()\n",
        "df_eval[\"success\"] = (df_eval[\"verdict\"] == \"YES\").astype(int)\n",
        "\n",
        "def print_section(title):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(title)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Overall success rate\n",
        "# ---------------------------------------------------------------------------\n",
        "overall_success = df_eval[\"success\"].mean()\n",
        "print_section(\"OVERALL SUCCESS RATE\")\n",
        "print(f\"Overall success rate: {overall_success:.3f} ({overall_success*100:.1f}%)\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Hardest/Easiest Rubrics\n",
        "# ---------------------------------------------------------------------------\n",
        "rubric_stats = df_eval.groupby(\"rubric\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"HARDEST / EASIEST RUBRICS\")\n",
        "print(\"Bottom 5 (hardest):\")\n",
        "print(rubric_stats.head(5).to_string())\n",
        "print(\"\\nTop 5 (easiest):\")\n",
        "print(rubric_stats.tail(5).to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Hardest/Easiest Graph Types\n",
        "# ---------------------------------------------------------------------------\n",
        "graph_stats = df_eval.groupby(\"graph_type\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"HARDEST / EASIEST GRAPH TYPES\")\n",
        "print(\"Hardest graph types:\")\n",
        "print(graph_stats.head().to_string())\n",
        "print(\"\\nEasiest graph types:\")\n",
        "print(graph_stats.tail().to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Best Performing Model\n",
        "# ---------------------------------------------------------------------------\n",
        "model_stats = df_eval.groupby(\"model\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"BEST PERFORMING MODELS\")\n",
        "print(model_stats.to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Prompt Type Effects\n",
        "# ---------------------------------------------------------------------------\n",
        "prompt_stats = df_eval.groupby(\"prompt_type\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"PROMPT TYPE EFFECTS\")\n",
        "print(prompt_stats.to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Chi-Square Test: Does prompt type matter?\n",
        "# ---------------------------------------------------------------------------\n",
        "cont_table = pd.crosstab(df_eval[\"prompt_type\"], df_eval[\"success\"])\n",
        "chi2, p, dof, expected = chi2_contingency(cont_table)\n",
        "\n",
        "print_section(\"STATISTICAL SIGNIFICANCE: PROMPT TYPE EFFECT\")\n",
        "print(\"Chi-square test on prompt_type vs success:\")\n",
        "print(f\"Chi² = {chi2:.3f}, df = {dof}, p = {p:.5f}\")\n",
        "\n",
        "if p < 0.05:\n",
        "    print(\"✅ Prompt type has a statistically significant effect on success.\")\n",
        "else:\n",
        "    print(\"❌ No statistically significant evidence that prompt type affects success.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP5uTpFclHtq",
        "outputId": "62f95339-75e1-4e80-e022-e5da8e3eb6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Success rate by model–prompt pair:\n",
            "                         mean  count\n",
            "model   prompt_type                 \n",
            "gemini  baseline     0.725000    120\n",
            "        standards    0.725000    120\n",
            "grok    standards    0.716667    120\n",
            "        selfcheck    0.700000    120\n",
            "chatgpt selfcheck    0.691667    120\n",
            "gemini  selfcheck    0.675000    120\n",
            "chatgpt standards    0.675000    120\n",
            "        baseline     0.650000    120\n",
            "grok    baseline     0.600000    120\n",
            "\n",
            "Best prompt–model pair:\n",
            "Model: gemini\n",
            "Prompt type: baseline\n",
            "Success rate: 0.725 (72.5%)\n",
            "Sample size: 120.0\n"
          ]
        }
      ],
      "source": [
        "# Compute success rate by (model, prompt_type)\n",
        "pair_stats = (\n",
        "    df_eval\n",
        "    .groupby([\"model\", \"prompt_type\"])[\"success\"]\n",
        "    .agg([\"mean\", \"count\"])\n",
        "    .sort_values(by=\"mean\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nSuccess rate by model–prompt pair:\")\n",
        "print(pair_stats)\n",
        "\n",
        "# Extract top-performing combination\n",
        "best_pair = pair_stats.iloc[0]\n",
        "best_model, best_prompt = pair_stats.index[0]\n",
        "\n",
        "print(\"\\nBest prompt–model pair:\")\n",
        "print(f\"Model: {best_model}\")\n",
        "print(f\"Prompt type: {best_prompt}\")\n",
        "print(f\"Success rate: {best_pair['mean']:.3f} ({best_pair['mean']*100:.1f}%)\")\n",
        "print(f\"Sample size: {best_pair['count']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
