{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZxgyXj3Ocb-1"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os, re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def save_df(df: pd.DataFrame, name: str, directory: str = \".\") -> None:\n",
        "    # ensure directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # create full path\n",
        "    filename = os.path.join(directory, f\"{name}.csv\")\n",
        "\n",
        "    try:\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"✅ Saved DataFrame to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to save DataFrame: {e}\")\n",
        "\n",
        "\n",
        "def load_csv_to_df(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load a CSV file into a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_path}' not found.\")\n",
        "        sys.exit(1)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: File '{csv_path}' is empty.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def reshape_rubric(df):\n",
        "    # rename rubric column\n",
        "    df = df.rename(columns={df.columns[0]: \"rubric\"})\n",
        "\n",
        "    # known model order cycling through triplets\n",
        "    models = [\"gemini\", \"grok\", \"chatgpt\"]\n",
        "\n",
        "    # extract graph types from non-\"Unnamed\" columns\n",
        "    graph_cols = [c for c in df.columns if not re.match(r\"Unnamed\", c) and c != \"rubric\"]\n",
        "\n",
        "    long_rows = []\n",
        "\n",
        "    for graph in graph_cols:\n",
        "        # find this graph's 3 column window (graph + next 2 cols)\n",
        "        idx = df.columns.get_loc(graph)\n",
        "        triplet = df.columns[idx:idx+3]  # assumes consistent structure\n",
        "\n",
        "        for model, col in zip(models, triplet):\n",
        "            temp = df[[\"rubric\", col]].copy()\n",
        "            temp[\"graph_type\"] = graph\n",
        "            temp[\"model\"] = model\n",
        "            temp = temp.rename(columns={col: \"value\"})\n",
        "            long_rows.append(temp)\n",
        "\n",
        "    tidy_df = pd.concat(long_rows, ignore_index=True)\n",
        "    return tidy_df\n",
        "\n",
        "def explode_prompt_types(df):\n",
        "    prompt_types = [\"baseline\", \"selfcheck\", \"standards\"]\n",
        "\n",
        "    verdict_map = {\n",
        "        \"y\": \"YES\",\n",
        "        \"n\": \"NO\",\n",
        "        \"-\": \"N/A\"\n",
        "    }\n",
        "\n",
        "    long_rows = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        val = str(row[\"value\"])  # original value column\n",
        "\n",
        "        if len(val) != 3:\n",
        "            raise ValueError(f\"Value '{val}' does not have 3 characters.\")\n",
        "\n",
        "        for char, pt in zip(val.lower(), prompt_types):  # lower-case normalize\n",
        "            if char not in verdict_map:\n",
        "                raise ValueError(f\"Unexpected verdict character '{char}' in '{val}'\")\n",
        "\n",
        "            new_row = row.copy()\n",
        "            new_row[\"verdict\"] = verdict_map[char]\n",
        "            new_row[\"prompt_type\"] = pt\n",
        "            new_row = new_row.drop(labels=[\"value\"])  # remove old column\n",
        "            long_rows.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(long_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2qNJ36zc5nX",
        "outputId": "c89605bc-d3d8-4b6a-dbf2-3bb8d634eece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               rubric    graph_type    model prompt_type  \\\n",
            "0     Code validity & reproducibility          Bars   gemini    baseline   \n",
            "1     Code validity & reproducibility          Bars   gemini   selfcheck   \n",
            "2     Code validity & reproducibility          Bars   gemini   standards   \n",
            "3        Encoding choice matches task          Bars   gemini    baseline   \n",
            "4        Encoding choice matches task          Bars   gemini   selfcheck   \n",
            "...                               ...           ...      ...         ...   \n",
            "1003            Task intent adherence  stacked bars  chatgpt   selfcheck   \n",
            "1004            Task intent adherence  stacked bars  chatgpt   standards   \n",
            "1005             Holistic readability  stacked bars  chatgpt    baseline   \n",
            "1006             Holistic readability  stacked bars  chatgpt   selfcheck   \n",
            "1007             Holistic readability  stacked bars  chatgpt   standards   \n",
            "\n",
            "     verdict  \n",
            "0        YES  \n",
            "1        YES  \n",
            "2        YES  \n",
            "3        YES  \n",
            "4        YES  \n",
            "...      ...  \n",
            "1003     YES  \n",
            "1004     YES  \n",
            "1005     YES  \n",
            "1006     YES  \n",
            "1007     YES  \n",
            "\n",
            "[1008 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "df = load_csv_to_df(\"LLM_Plotting_Rubric.csv\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl6dz5JGdDbZ",
        "outputId": "39c3c703-28b9-43f5-f28e-1bd0032539b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved DataFrame to ./LLM_Plotting_Rubric.csv\n"
          ]
        }
      ],
      "source": [
        "save_df(df, \"LLM_Plotting_Rubric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSW6ZgS5heTv",
        "outputId": "b9a35e7f-c89a-4ddf-d12a-2128bf43a42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "OVERALL SUCCESS RATE\n",
            "================================================================================\n",
            "Overall success rate: 0.733 (73.3%)\n",
            "\n",
            "================================================================================\n",
            "HARDEST / EASIEST RUBRICS\n",
            "================================================================================\n",
            "Bottom 5 (hardest):\n",
            "rubric\n",
            "Axis integrity                     0.361111\n",
            "Legend–encoding alignment          0.402778\n",
            "Redundancy                         0.500000\n",
            "Appropriate Data‑ink ratio         0.652778\n",
            "Scale consistency across panels    0.694444\n",
            "\n",
            "Top 5 (easiest):\n",
            "rubric\n",
            "Gridlines/ticks & sizing           0.805556\n",
            "Data faithfulness                  0.847222\n",
            "Code validity & reproducibility    0.972222\n",
            "Encoding choice matches task       0.972222\n",
            "Holistic readability               0.972222\n",
            "\n",
            "================================================================================\n",
            "HARDEST / EASIEST GRAPH TYPES\n",
            "================================================================================\n",
            "Hardest graph types:\n",
            "graph_type\n",
            "dual axis       0.539683\n",
            "small multi     0.619048\n",
            "heatmap         0.674603\n",
            "line gaps       0.746032\n",
            "stacked bars    0.777778\n",
            "\n",
            "Easiest graph types:\n",
            "graph_type\n",
            "line gaps        0.746032\n",
            "stacked bars     0.777778\n",
            "scatter group    0.817460\n",
            "Bars             0.833333\n",
            "histogram        0.857143\n",
            "\n",
            "================================================================================\n",
            "BEST PERFORMING MODELS\n",
            "================================================================================\n",
            "model\n",
            "chatgpt    0.720238\n",
            "grok       0.720238\n",
            "gemini     0.758929\n",
            "\n",
            "================================================================================\n",
            "PROMPT TYPE EFFECTS\n",
            "================================================================================\n",
            "prompt_type\n",
            "baseline     0.705357\n",
            "selfcheck    0.738095\n",
            "standards    0.755952\n",
            "\n",
            "================================================================================\n",
            "STATISTICAL SIGNIFICANCE: PROMPT TYPE EFFECT\n",
            "================================================================================\n",
            "Chi-square test on prompt_type vs success:\n",
            "Chi² = 2.262, df = 2, p = 0.32279\n",
            "❌ No statistically significant evidence that prompt type affects success.\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Convert verdict to binary success for analysis (ignore N/A)\n",
        "df_eval = df[df[\"verdict\"] != \"N/A\"].copy()\n",
        "df_eval[\"success\"] = (df_eval[\"verdict\"] == \"YES\").astype(int)\n",
        "\n",
        "def print_section(title):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(title)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Overall success rate\n",
        "# ---------------------------------------------------------------------------\n",
        "overall_success = df_eval[\"success\"].mean()\n",
        "print_section(\"OVERALL SUCCESS RATE\")\n",
        "print(f\"Overall success rate: {overall_success:.3f} ({overall_success*100:.1f}%)\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Hardest/Easiest Rubrics\n",
        "# ---------------------------------------------------------------------------\n",
        "rubric_stats = df_eval.groupby(\"rubric\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"HARDEST / EASIEST RUBRICS\")\n",
        "print(\"Bottom 5 (hardest):\")\n",
        "print(rubric_stats.head(5).to_string())\n",
        "print(\"\\nTop 5 (easiest):\")\n",
        "print(rubric_stats.tail(5).to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Hardest/Easiest Graph Types\n",
        "# ---------------------------------------------------------------------------\n",
        "graph_stats = df_eval.groupby(\"graph_type\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"HARDEST / EASIEST GRAPH TYPES\")\n",
        "print(\"Hardest graph types:\")\n",
        "print(graph_stats.head().to_string())\n",
        "print(\"\\nEasiest graph types:\")\n",
        "print(graph_stats.tail().to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Best Performing Model\n",
        "# ---------------------------------------------------------------------------\n",
        "model_stats = df_eval.groupby(\"model\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"BEST PERFORMING MODELS\")\n",
        "print(model_stats.to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Prompt Type Effects\n",
        "# ---------------------------------------------------------------------------\n",
        "prompt_stats = df_eval.groupby(\"prompt_type\")[\"success\"].mean().sort_values()\n",
        "\n",
        "print_section(\"PROMPT TYPE EFFECTS\")\n",
        "print(prompt_stats.to_string())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Chi-Square Test: Does prompt type matter?\n",
        "# ---------------------------------------------------------------------------\n",
        "cont_table = pd.crosstab(df_eval[\"prompt_type\"], df_eval[\"success\"])\n",
        "chi2, p, dof, expected = chi2_contingency(cont_table)\n",
        "\n",
        "print_section(\"STATISTICAL SIGNIFICANCE: PROMPT TYPE EFFECT\")\n",
        "print(\"Chi-square test on prompt_type vs success:\")\n",
        "print(f\"Chi² = {chi2:.3f}, df = {dof}, p = {p:.5f}\")\n",
        "\n",
        "if p < 0.05:\n",
        "    print(\"✅ Prompt type has a statistically significant effect on success.\")\n",
        "else:\n",
        "    print(\"❌ No statistically significant evidence that prompt type affects success.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP5uTpFclHtq",
        "outputId": "62f95339-75e1-4e80-e022-e5da8e3eb6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Success rate by model–prompt pair:\n",
            "                         mean  count\n",
            "model   prompt_type                 \n",
            "gemini  baseline     0.776786    112\n",
            "        standards    0.776786    112\n",
            "grok    standards    0.767857    112\n",
            "        selfcheck    0.750000    112\n",
            "chatgpt selfcheck    0.741071    112\n",
            "gemini  selfcheck    0.723214    112\n",
            "chatgpt standards    0.723214    112\n",
            "        baseline     0.696429    112\n",
            "grok    baseline     0.642857    112\n",
            "\n",
            "Best prompt–model pair:\n",
            "Model: gemini\n",
            "Prompt type: baseline\n",
            "Success rate: 0.777 (77.7%)\n",
            "Sample size: 112.0\n"
          ]
        }
      ],
      "source": [
        "# Compute success rate by (model, prompt_type)\n",
        "pair_stats = (\n",
        "    df_eval\n",
        "    .groupby([\"model\", \"prompt_type\"])[\"success\"]\n",
        "    .agg([\"mean\", \"count\"])\n",
        "    .sort_values(by=\"mean\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nSuccess rate by model–prompt pair:\")\n",
        "print(pair_stats)\n",
        "\n",
        "# Extract top-performing combination\n",
        "best_pair = pair_stats.iloc[0]\n",
        "best_model, best_prompt = pair_stats.index[0]\n",
        "\n",
        "print(\"\\nBest prompt–model pair:\")\n",
        "print(f\"Model: {best_model}\")\n",
        "print(f\"Prompt type: {best_prompt}\")\n",
        "print(f\"Success rate: {best_pair['mean']:.3f} ({best_pair['mean']*100:.1f}%)\")\n",
        "print(f\"Sample size: {best_pair['count']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Wrote tables to: metrics\n",
            "✅ Wrote figures to: figs\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# I/O\n",
        "# --------------------------------------------------------------------------------------\n",
        "LINTER_CSV    = \"lint_summary.csv\"\n",
        "VIOLATIONS_CSV= \"violations.csv\"\n",
        "\n",
        "OUT_TAB_DIR = \"metrics\"\n",
        "OUT_FIG_DIR = \"figs\"\n",
        "os.makedirs(OUT_TAB_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_FIG_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# Load\n",
        "# --------------------------------------------------------------------------------------\n",
        "linter_df = pd.read_csv(LINTER_CSV)\n",
        "viol_df   = pd.read_csv(VIOLATIONS_CSV)\n",
        "\n",
        "# Minimal cleanup: normalize labels\n",
        "def norm(s): return str(s).strip().lower().replace(\" \", \"\").replace(\"-\", \"_\")\n",
        "linter_df[\"task\"]      = linter_df[\"task\"].astype(str)\n",
        "linter_df[\"model\"]     = linter_df[\"model\"].astype(str)\n",
        "linter_df[\"condition\"] = linter_df[\"condition\"].astype(str)\n",
        "linter_df[\"rule\"]      = linter_df[\"rule\"].astype(str)\n",
        "linter_df[\"status\"]    = linter_df[\"status\"].astype(str)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 1) “Gates” / objective heuristics from linter (partial, no s2)\n",
        "#    G2 baseline_zero_bar; G3 no dual_axes; + useful heuristics (labels, legend, contrast)\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Helper: status -> violation (1 if warn/fail, else 0)\n",
        "VIOL_STATUSES = {\"warn\", \"fail\"}\n",
        "def is_viol(row):\n",
        "    return 1 if str(row[\"status\"]).lower() in VIOL_STATUSES else 0\n",
        "\n",
        "linter_df[\"viol\"] = linter_df.apply(is_viol, axis=1)\n",
        "\n",
        "# Gate-like rules we can quantify directly from linter\n",
        "GATE_RULES = {\n",
        "    \"baseline_zero_bar\": \"Bars baseline at zero (or justified)\",  # warn/fail => violation\n",
        "    \"dual_axes\":         \"No unjustified dual axes\",              # fail => violation\n",
        "}\n",
        "\n",
        "# Useful non-gate heuristics for the paper\n",
        "AUX_RULES = {\n",
        "    \"labels_present\": \"Title + X + Y labels present\",\n",
        "    \"legend_call\":    \"Legend present when needed\",\n",
        "    \"contrast_text\":  \"Text contrast >= threshold\",\n",
        "}\n",
        "\n",
        "KEEP_RULES = list(GATE_RULES.keys()) + list(AUX_RULES.keys())\n",
        "\n",
        "sub = linter_df[linter_df[\"rule\"].isin(KEEP_RULES)].copy()\n",
        "\n",
        "# Group to violation rate by (model, condition, rule)\n",
        "viol_rate = (\n",
        "    sub.groupby([\"model\", \"condition\", \"rule\"])[\"viol\"]\n",
        "       .mean()\n",
        "       .reset_index()\n",
        "       .rename(columns={\"viol\":\"violation_rate\"})\n",
        ")\n",
        "\n",
        "# Save table\n",
        "viol_rate.sort_values([\"rule\",\"model\",\"condition\"]).to_csv(\n",
        "    os.path.join(OUT_TAB_DIR, \"violation_rate_by_model_condition_rule.csv\"),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "# Also compute pass rate (1 - violation_rate)\n",
        "pass_rate = viol_rate.copy()\n",
        "pass_rate[\"pass_rate\"] = 1 - pass_rate[\"violation_rate\"]\n",
        "pass_rate.to_csv(os.path.join(OUT_TAB_DIR, \"pass_rate_by_model_condition_rule.csv\"), index=False)\n",
        "\n",
        "# Pivot for a heatmap: rule x (model,condition) → violation rate\n",
        "heat_piv = viol_rate.pivot_table(index=\"rule\", columns=[\"model\",\"condition\"], values=\"violation_rate\")\n",
        "heat_piv.to_csv(os.path.join(OUT_TAB_DIR, \"violation_heatmap_table.csv\"))\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 2) Prompt-gain by condition (baseline→standards; standards→selfcheck) per rule and per model\n",
        "# --------------------------------------------------------------------------------------\n",
        "pg = viol_rate.pivot_table(index=[\"rule\",\"model\"], columns=\"condition\", values=\"violation_rate\")\n",
        "for col in (\"baseline\",\"standards\",\"selfcheck\"):\n",
        "    if col not in pg.columns: pg[col] = np.nan\n",
        "\n",
        "pg[\"gain_baseline_to_standards\"] = pg[\"baseline\"] - pg[\"standards\"]  # positive = fewer violations under standards\n",
        "pg[\"gain_standards_to_selfcheck\"] = pg[\"standards\"] - pg[\"selfcheck\"]\n",
        "pg = pg.reset_index()\n",
        "pg.to_csv(os.path.join(OUT_TAB_DIR, \"prompt_gain_by_rule_model.csv\"), index=False)\n",
        "\n",
        "# Also aggregate across models for a rule-level view\n",
        "pg_rule = (\n",
        "    pg.groupby(\"rule\")[[\"gain_baseline_to_standards\",\"gain_standards_to_selfcheck\"]]\n",
        "      .mean()\n",
        "      .sort_values(\"gain_baseline_to_standards\", ascending=False)\n",
        "      .reset_index()\n",
        ")\n",
        "pg_rule.to_csv(os.path.join(OUT_TAB_DIR, \"prompt_gain_by_rule_mean.csv\"), index=False)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 3) Contrast distributions (WCAG-like) by model and by condition\n",
        "# --------------------------------------------------------------------------------------\n",
        "contrast = linter_df[linter_df[\"rule\"]==\"contrast_text\"].copy()\n",
        "# Keep only rows with numeric ratio\n",
        "contrast = contrast[pd.to_numeric(contrast[\"ratio\"], errors=\"coerce\").notna()]\n",
        "contrast[\"ratio\"] = contrast[\"ratio\"].astype(float)\n",
        "\n",
        "# Table: % below thresholds by (model, condition)\n",
        "def below_thresh_rate(group, thr):\n",
        "    return (group[\"ratio\"] < thr).mean()\n",
        "\n",
        "rows = []\n",
        "for (m, c), grp in contrast.groupby([\"model\",\"condition\"]):\n",
        "    rows.append({\"model\": m, \"condition\": c,\n",
        "                 \"pct_below_4p5\": below_thresh_rate(grp, 4.5),\n",
        "                 \"pct_below_3p0\": below_thresh_rate(grp, 3.0),\n",
        "                 \"n\": len(grp)})\n",
        "contrast_summary = pd.DataFrame(rows).sort_values([\"model\",\"condition\"])\n",
        "contrast_summary.to_csv(os.path.join(OUT_TAB_DIR, \"contrast_summary.csv\"), index=False)\n",
        "\n",
        "# Figure: boxplot of contrast ratios by model (collapsing conditions) and by condition (collapsing models)\n",
        "def boxplot_grouped(df, group_col, value_col, title, outfile):\n",
        "    order = sorted(df[group_col].unique())\n",
        "    data = [df[df[group_col]==g][value_col].values for g in order]\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.boxplot(data, tick_labels=order, showmeans=True)\n",
        "    ax.axhline(4.5, linestyle=\"--\")  # text threshold (reference)\n",
        "    ax.axhline(3.0, linestyle=\":\")   # non-text threshold (reference)\n",
        "    ax.set_ylabel(\"Contrast ratio\")\n",
        "    ax.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(outfile, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "boxplot_grouped(\n",
        "    contrast, \"model\", \"ratio\",\n",
        "    \"Contrast ratio by model (all conditions)\",\n",
        "    os.path.join(OUT_FIG_DIR, \"contrast_by_model.png\")\n",
        ")\n",
        "boxplot_grouped(\n",
        "    contrast, \"condition\", \"ratio\",\n",
        "    \"Contrast ratio by prompt condition (all models)\",\n",
        "    os.path.join(OUT_FIG_DIR, \"contrast_by_condition.png\")\n",
        ")\n",
        "# Pretty boxplot of contrast ratios by model with WCAG legend\n",
        "# Expects a DataFrame `contrast` with columns: model, condition, ratio (float)\n",
        "\n",
        "def pretty_contrast_boxplot_by_model(df: pd.DataFrame,\n",
        "                                     value_col: str = \"ratio\",\n",
        "                                     group_col: str = \"model\",\n",
        "                                     title: str = \"Contrast ratio by model (all conditions)\",\n",
        "                                     outfile: str = \"reports/figs/contrast_by_model_pretty.png\",\n",
        "                                     sort_desc: bool = True,  # sort by median (desc = best on the right)\n",
        "                                     display_name_map=None\n",
        "                                     ):\n",
        "    # Order groups alphabetically (or switch to median/mean order if preferred)\n",
        "    # --- order models by median contrast ---\n",
        "    med = df.groupby(group_col)[value_col].median().sort_values(ascending=not sort_desc)\n",
        "    order = med.index.tolist()\n",
        "\n",
        "    # pretty names only for display\n",
        "    if display_name_map is None: \n",
        "        display_name_map = {}\n",
        "    labels = [display_name_map.get(g, g) for g in order]\n",
        "\n",
        "    # --- gather data and sample sizes in the chosen order ---\n",
        "    data = [df[df[group_col] == g][value_col].dropna().values for g in order]\n",
        "    ns   = [len(x) for x in data]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(9.5, 4.8))\n",
        "\n",
        "    # --- styling props so legend matches what we draw ---\n",
        "    meanprops   = dict(marker=\"D\", markersize=6, markerfacecolor=\"tab:blue\",\n",
        "                       markeredgecolor=\"white\")\n",
        "    medianprops = dict(color=\"tab:orange\", linewidth=2)\n",
        "    boxprops    = dict(linewidth=1.2)\n",
        "    whiskerprops= dict(linewidth=1.0)\n",
        "    capprops    = dict(linewidth=1.0)\n",
        "    flierprops  = dict(marker=\"o\", markersize=4, markerfacecolor=\"none\",\n",
        "                       markeredgecolor=\"0.4\", alpha=0.7)\n",
        "\n",
        "    bp = ax.boxplot(\n",
        "        data,\n",
        "        tick_labels=labels,\n",
        "        vert=True,\n",
        "        showmeans=True, # show a mean marker per box\n",
        "        meanline=False, # use a marker rather than a line\n",
        "        meanprops=meanprops,\n",
        "        medianprops=medianprops,\n",
        "        boxprops=boxprops,\n",
        "        whiskerprops=whiskerprops,\n",
        "        capprops=capprops,\n",
        "        flierprops=flierprops,\n",
        "        manage_ticks=True\n",
        "    )\n",
        "\n",
        "    # Axis, grid, and limits: start at 0 for interpretability wrt thresholds\n",
        "    ymax = max([x.max() if len(x) else 0 for x in data] + [5.0]) * 1.15\n",
        "    ax.set_ylim(0, ymax)                          # start y at 0\n",
        "    ax.yaxis.grid(True, linestyle=\":\", linewidth=0.8, alpha=0.8)\n",
        "    ax.set_ylabel(\"Contrast ratio\")\n",
        "    ax.set_title(title, pad=10)\n",
        "\n",
        "    # WCAG reference lines\n",
        "    line_text    = ax.axhline(4.5, linestyle=\"--\", linewidth=1.4, color=\"tab:blue\")\n",
        "    line_nontext = ax.axhline(3.0, linestyle=\":\",  linewidth=1.4, color=\"tab:blue\")\n",
        "\n",
        "    # Sample size under each tick\n",
        "    y0, y1 = ax.get_ylim()\n",
        "    for xtick, n in zip(ax.get_xticks(), ns):\n",
        "        ax.text(xtick, y0 + 0.02*(y1 - y0), f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
        "\n",
        "    # Legend outside: thresholds + mean/median glyphs\n",
        "    legend_handles = [\n",
        "        Line2D([0], [0], linestyle=\"--\", color=\"tab:blue\", linewidth=1.4, label=\"WCAG text 4.5:1\"),\n",
        "        Line2D([0], [0], linestyle=\":\",  color=\"tab:blue\", linewidth=1.4, label=\"WCAG non-text 3:1\"),\n",
        "        Line2D([0], [0], marker=\"D\", color=\"w\", markerfacecolor=\"tab:blue\",\n",
        "               markeredgecolor=\"white\", markersize=6, linestyle=\"None\", label=\"Mean\"),\n",
        "        Line2D([0], [0], color=\"tab:orange\", linewidth=2, label=\"Median\"), # median line is drawn inside the box\n",
        "    ]\n",
        "    ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1.02, 1),\n",
        "              borderaxespad=0., frameon=False)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(outfile, dpi=300, bbox_inches=\"tight\")  # tight to include outside legend\n",
        "    plt.close(fig)\n",
        "\n",
        "pretty_contrast_boxplot_by_model(\n",
        "    contrast,\n",
        "    outfile=os.path.join(OUT_FIG_DIR, \"contrast_by_model_pretty.png\"),\n",
        "    display_name_map={\"gemini25pro\": \"gemini2.5pro\"}\n",
        ")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 4) Rule × task heatmap: where violations concentrate (supports RQ2)\n",
        "# --------------------------------------------------------------------------------------\n",
        "viol_task = (\n",
        "    sub.groupby([\"task\",\"rule\"])[\"viol\"]\n",
        "       .mean()\n",
        "       .reset_index()\n",
        "       .rename(columns={\"viol\":\"violation_rate\"})\n",
        ")\n",
        "# Save table\n",
        "viol_task.to_csv(os.path.join(OUT_TAB_DIR, \"violation_rate_by_task_rule.csv\"), index=False)\n",
        "\n",
        "# Heatmap figure (rule x task)\n",
        "hm = viol_task.pivot_table(index=\"rule\", columns=\"task\", values=\"violation_rate\")\n",
        "# Simple imshow heatmap\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "im = ax.imshow(hm.values, aspect=\"auto\")\n",
        "ax.set_yticks(range(hm.shape[0])); ax.set_yticklabels(hm.index)\n",
        "ax.set_xticks(range(hm.shape[1])); ax.set_xticklabels(hm.columns, rotation=45, ha=\"right\")\n",
        "ax.set_title(\"Violation rates by rule × task (lower is better)\")\n",
        "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(OUT_FIG_DIR, \"violation_heatmap_rule_by_task.png\"), dpi=200)\n",
        "plt.close(fig)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 5) Gate-like pass rates by model (bar chart)\n",
        "# --------------------------------------------------------------------------------------\n",
        "gate_like = sub[sub[\"rule\"].isin(GATE_RULES.keys())].copy()\n",
        "gate_like[\"pass\"] = 1 - gate_like[\"viol\"]\n",
        "gate_model = gate_like.groupby([\"model\",\"rule\"])[\"pass\"].mean().reset_index()\n",
        "\n",
        "# Table\n",
        "gate_model_piv = gate_model.pivot_table(index=\"rule\", columns=\"model\", values=\"pass\")\n",
        "gate_model_piv.to_csv(os.path.join(OUT_TAB_DIR, \"gate_like_pass_rate_by_model.csv\"))\n",
        "\n",
        "# Figure: bars per rule (grouped by model)\n",
        "rules_order = list(gate_model[\"rule\"].unique())\n",
        "models_order = sorted(gate_model[\"model\"].unique())\n",
        "x = np.arange(len(rules_order))\n",
        "width = 0.8 / max(1, len(models_order))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,4))\n",
        "for i, m in enumerate(models_order):\n",
        "    vals = [gate_model[(gate_model[\"rule\"]==r) & (gate_model[\"model\"]==m)][\"pass\"].mean() for r in rules_order]\n",
        "    ax.bar(x + i*width, vals, width, label=m)\n",
        "ax.set_xticks(x + width*(len(models_order)-1)/2)\n",
        "ax.set_xticklabels(rules_order, rotation=30, ha=\"right\")\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_ylabel(\"Pass rate\")\n",
        "ax.set_title(\"Gate-like pass rates by model\")\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(OUT_FIG_DIR, \"gate_like_pass_by_model.png\"), dpi=200)\n",
        "plt.close(fig)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 6) Top violations (from violations.csv) — counts and normalized rates\n",
        "# --------------------------------------------------------------------------------------\n",
        "# Raw counts already in violations.csv\n",
        "viol_df.to_csv(os.path.join(OUT_TAB_DIR, \"top_violations_raw_counts.csv\"), index=False)\n",
        "\n",
        "# Normalize by opportunity: approximate denominator = number of trials where rule was applicable\n",
        "# We estimate applicability as: count of trials that have that rule at all in linter_df\n",
        "denom = (linter_df.groupby([\"task\",\"model\",\"condition\",\"rule\"])[\"trial_id\"]\n",
        "                 .nunique()\n",
        "                 .groupby([\"rule\"])\n",
        "                 .sum()\n",
        "                 .rename(\"applicable_trials\")\n",
        "                 .reset_index())\n",
        "\n",
        "viol_norm = (viol_df.groupby(\"rule\")[\"violations\"].sum().reset_index()\n",
        "             .merge(denom, on=\"rule\", how=\"left\"))\n",
        "viol_norm[\"violations_per_100_trials\"] = 100 * viol_norm[\"violations\"] / viol_norm[\"applicable_trials\"].clip(lower=1)\n",
        "viol_norm.sort_values(\"violations_per_100_trials\", ascending=False, inplace=True)\n",
        "viol_norm.to_csv(os.path.join(OUT_TAB_DIR, \"top_violations_normalized.csv\"), index=False)\n",
        "\n",
        "# Simple bar plot of normalized top-10\n",
        "top10 = viol_norm.head(10)\n",
        "fig, ax = plt.subplots(figsize=(7,4))\n",
        "ax.barh(top10[\"rule\"][::-1], top10[\"violations_per_100_trials\"][::-1])\n",
        "ax.set_xlabel(\"Violations per 100 applicable trials\")\n",
        "ax.set_title(\"Top violation rules (normalized)\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(OUT_FIG_DIR, \"top_violations_normalized.png\"), dpi=200)\n",
        "plt.close(fig)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# 7) Summaries for manuscript tables\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# (a) Model × condition overall pass rate on selected rules\n",
        "sel_rules = [\"baseline_zero_bar\",\"dual_axes\",\"labels_present\",\"legend_call\",\"contrast_text\"]\n",
        "overall = (\n",
        "    sub[sub[\"rule\"].isin(sel_rules)]\n",
        "    .assign(pass_flag=lambda d: 1 - d[\"viol\"])\n",
        "    .groupby([\"model\",\"condition\"])[\"pass_flag\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"pass_flag\":\"overall_pass_rate_on_selected_rules\"})\n",
        ")\n",
        "overall.to_csv(os.path.join(OUT_TAB_DIR, \"overall_selected_pass_by_model_condition.csv\"), index=False)\n",
        "\n",
        "# (b) Prompt-gain table (mean across models) for selected rules\n",
        "pg_sel = pg[pg[\"rule\"].isin(sel_rules)].copy()\n",
        "pg_sel_mean = (\n",
        "    pg_sel.groupby(\"rule\")[[\"gain_baseline_to_standards\",\"gain_standards_to_selfcheck\"]]\n",
        "          .mean()\n",
        "          .reset_index()\n",
        "          .sort_values(\"gain_baseline_to_standards\", ascending=False)\n",
        ")\n",
        "pg_sel_mean.to_csv(os.path.join(OUT_TAB_DIR, \"prompt_gain_selected_rules_mean.csv\"), index=False)\n",
        "\n",
        "print(\"✅ Wrote tables to:\", OUT_TAB_DIR)\n",
        "print(\"✅ Wrote figures to:\", OUT_FIG_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
